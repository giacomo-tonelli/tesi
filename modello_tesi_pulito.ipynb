{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an-fIZ1mDD2o"
      },
      "outputs": [],
      "source": [
        "#CAMBIARE RUN TIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnxNA2PIrrr4",
        "outputId": "cb14b714-5707-44e9-d880-2c3a5b8efd66"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ySL-uLv7tSs"
      },
      "source": [
        "riavviare runtime dopo l installazione di bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHst3kz1qH7Q",
        "outputId": "44147947-17ed-4992-d4ba-da92a0024c9f"
      },
      "outputs": [],
      "source": [
        "# --- Blocco 1: Setup Iniziale per Mistral-7B-Instruct-v0.2 su GPU ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import threading\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "import torch\n",
        "\n",
        "# 1. Montare Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount può essere utile\n",
        "    print(\"Google Drive montato con successo!\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il montaggio di Google Drive: {e}\")\n",
        "    raise SystemExit(\"Impossibile montare Google Drive. Verifica i permessi.\")\n",
        "\n",
        "# 2. Definire il percorso del modello salvato su Drive e configura la quantizzazione\n",
        "\n",
        "model_path_on_drive = \"/content/drive/MyDrive/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Controllo disponibilità CUDA (GPU)\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"ERRORE: GPU non disponibile. Assicurati di aver selezionato un runtime con GPU (Runtime -> Change runtime type).\")\n",
        "    raise SystemExit(\"GPU non trovata.\")\n",
        "\n",
        "# Configurazione per la quantizzazione a 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16 # Cambiato a float16 come suggerito per compatibilità GPU\n",
        ")\n",
        "print(f\"Tentativo di caricare il modello da: {model_path_on_drive} con quantizzazione a 4-bit su GPU.\")\n",
        "\n",
        "# 3. Carica Tokenizer e Modello su GPU\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path_on_drive)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(\"Tokenizer caricato e pad_token impostato.\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path_on_drive,\n",
        "        quantization_config=bnb_config, # quantizzazione\n",
        "        device_map=\"auto\",  # accelerate gestisce il mapping sulla GPU\n",
        "        torch_dtype=torch.float16 # Dovrebbe corrispondere a bnb_4bit_compute_dtype\n",
        "                                   # anche qui provare torch.float16 se bfloat16 da problemi\n",
        "    )\n",
        "    model.eval() # Imposta il modello in modalità valutazione\n",
        "\n",
        "    # Verifica dispositivo\n",
        "    if hasattr(model, 'device'):\n",
        "        print(f\"Modello caricato. Dispositivo principale del modello: {model.device}\")\n",
        "        if len(list(model.parameters())) > 0:\n",
        "            param_device = next(model.parameters()).device\n",
        "            print(f\"Parametri del modello allocati su: {param_device}\")\n",
        "            if \"cuda\" not in str(param_device):\n",
        "                print(\"ATTENZIONE: Il modello potrebbe non essere sulla GPU come previsto!\")\n",
        "        else:\n",
        "             print(\"ATTENZIONE: Modello senza parametri?\")\n",
        "    else:\n",
        "        print(\"Modello caricato, ma l'attributo 'device' non è direttamente accessibile.\")\n",
        "\n",
        "\n",
        "except ImportError as e_imp:\n",
        "    print(f\"ImportError: {e_imp}\")\n",
        "    print(\"Questo errore spesso indica che 'bitsandbytes' non è installato correttamente o il kernel non è stato riavviato dopo l'installazione.\")\n",
        "    print(\"Prova ad aggiungere '!pip install -U bitsandbytes' all'inizio di questa cella, eseguila, POI RIAVVIA IL RUNTIME (Menu Runtime > Riavvia sessione) e riesegui questa cella.\")\n",
        "    raise SystemExit(\"Errore di importazione, probabilmente bitsandbytes.\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il caricamento del modello {model_path_on_drive} da Drive: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise SystemExit(\"Impossibile caricare il modello da Drive.\")\n",
        "\n",
        "print(\"\\n--- Setup GPU completato. 'model' e 'tokenizer' sono pronti. ---\")\n",
        "\n",
        "# --- Variabile globale per il prompt dell'utente ---\n",
        "user_prompt_template = \"\"\"\n",
        "You are an expert analyst specializing in patents and scientific literature.\n",
        "Your task is to read technical texts and distill their core technological concepts into clear, concise sentences.\n",
        "You excel at identifying the main purpose of an invention or research (Function), explaining how it works (Solution), and stating where it can be applied (Application).\n",
        "Your summaries are precise, scientifically accurate, and easily understandable to researchers and professionals.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Respond ONLY in the following JSON format and the response MUST start with {{ and end with }}:\n",
        "\n",
        "{{\n",
        "  \"function\": \"<brief summary of the main purpose, in verb-object form>\",\n",
        "  \"solution\": \"<brief technical explanation of how the purpose is achieved>\",\n",
        "  \"application\": \"<brief summary of practical or industrial domains where it applies>\"\n",
        "}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yreSpYowqRKh",
        "outputId": "189d0fc8-3627-4ebc-afca-aea009a8bc04"
      },
      "outputs": [],
      "source": [
        "# --- Blocco 2: Definizione delle Funzioni di Elaborazione ---\n",
        "\n",
        "# Verifica che le variabili dal setup siano presenti\n",
        "if 'model' not in globals() or 'tokenizer' not in globals() or 'user_prompt_template' not in globals():\n",
        "    print(\"ERRORE CRITICO: 'model', 'tokenizer', o 'user_prompt_template' non sono definiti.\")\n",
        "    print(\"Esegui prima la CELLA 1 (Setup Iniziale).\")\n",
        "    raise SystemExit(\"Setup del modello non completato.\")\n",
        "\n",
        "REQUIRED_KEYS_EXTRACT = {\"function\", \"solution\", \"application\"}\n",
        "def is_valid_output_extract(output): # Usata dopo in extract_concepts\n",
        "    if not isinstance(output, dict):\n",
        "        return False\n",
        "    return REQUIRED_KEYS_EXTRACT.issubset(output.keys())\n",
        "\n",
        "save_lock = threading.Lock() # Per la scrittura su file thread-safe\n",
        "\n",
        "def save_block_to_jsonl(results, path='results_patent.jsonl'):\n",
        "    with save_lock:\n",
        "        with open(path, 'a', encoding='utf-8') as f:\n",
        "            for r in results:\n",
        "                f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
        "\n",
        "def get_already_processed_patent_ids(jsonl_path):\n",
        "    if not os.path.exists(jsonl_path):\n",
        "        return set()\n",
        "    done = set()\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                if \"patent_id\" in obj:\n",
        "                    done.add(str(obj[\"patent_id\"]))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return done\n",
        "\n",
        "def is_valid_final_output(result): # Usata dopo in process_batch prima di salvare\n",
        "    if not isinstance(result, dict): return False\n",
        "    return all(k in result and isinstance(result[k], str) for k in [\"function\", \"solution\", \"application\", \"patent_id\"])\n",
        "\n",
        "# MODIFICA aggiunta per DEBUG: extract_concepts\n",
        "def extract_concepts(text):\n",
        "    global model, tokenizer, user_prompt_template # per assicurare l'uso delle variabili globali\n",
        "    chat_messages = [{\"role\": \"user\", \"content\": user_prompt_template.format(text=text)}]\n",
        "    try:\n",
        "        prompt_for_llm = tokenizer.apply_chat_template(chat_messages, tokenize=False, add_generation_prompt=True)\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG extract_concepts: Errore apply_chat_template: {e}\")\n",
        "        return None\n",
        "\n",
        "    inputs = tokenizer(prompt_for_llm, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[-1]:]\n",
        "    decoded_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "    #print(f\"\\nDEBUG decoded_output COMPLETO:\\n{decoded_output}\\n\")\n",
        "\n",
        "    json_match = re.search(r'\\{[\\s\\S]*\\}', decoded_output)\n",
        "    #json_match = re.search(r'\\{[\\s\\S]*?\\}', decoded_output)\n",
        "    cleaned_json_str = None\n",
        "    if json_match:\n",
        "        json_str_raw = json_match.group().strip()\n",
        "        if json_str_raw.startswith(\"```json\"):\n",
        "            json_str_raw = json_str_raw[len(\"```json\"):].strip()\n",
        "        if json_str_raw.endswith(\"```\"):\n",
        "            json_str_raw = json_str_raw[:-len(\"```\")].strip()\n",
        "        cleaned_json_str = json_str_raw\n",
        "\n",
        "    if cleaned_json_str:\n",
        "        try:\n",
        "            parsed_json = json.loads(cleaned_json_str)\n",
        "            if is_valid_output_extract(parsed_json):\n",
        "                print(\"DEBUG extract_concepts: JSON PARSATO E VALIDO (con function, solution, application).\")\n",
        "                return parsed_json\n",
        "            else:\n",
        "                print(\"DEBUG extract_concepts: JSON PARSATO MA NON VALIDO (mancano chiavi 'function', 'solution', o 'application').\")\n",
        "                print(f\"DEBUG extract_concepts: Chiavi trovate: {list(parsed_json.keys())}\")\n",
        "                return None\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"DEBUG extract_concepts: JSONDecodeError: '{e}'\")\n",
        "            print(f\"DEBUG extract_concepts: Stringa JSON (pulita) che ha causato l'errore (primi 200): '{cleaned_json_str[:200]}...'\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"DEBUG extract_concepts: Nessun blocco JSON trovato nell'output dell'LLM con regex.\")\n",
        "        return None\n",
        "\n",
        "def process_row(index, row):\n",
        "    global model, tokenizer # Anche se non usate direttamente qui, extract_concepts le usa\n",
        "    try:\n",
        "        text_parts = []\n",
        "        if \"appln_title\" in row and pd.notna(row[\"appln_title\"]): text_parts.append(str(row[\"appln_title\"]))\n",
        "        if \"appln_abstract\" in row and pd.notna(row[\"appln_abstract\"]): text_parts.append(str(row[\"appln_abstract\"]))\n",
        "        if not text_parts:\n",
        "            print(f\"DEBUG process_row: Riga {index} non ha titolo/abstract. Salto.\")\n",
        "            return None\n",
        "        text = \". \".join(text_parts)\n",
        "\n",
        "        result_from_llm = extract_concepts(text)\n",
        "        if result_from_llm is None: return None\n",
        "\n",
        "        if \"patent\" in row and pd.notna(row[\"patent\"]):\n",
        "            result_from_llm[\"patent_id\"] = str(row[\"patent\"])\n",
        "        else:\n",
        "            print(f\"DEBUG process_row: Riga {index} manca 'patent_id' (colonna 'patent' nel df). Scartata.\")\n",
        "            return None\n",
        "        return result_from_llm\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG process_row: Errore imprevisto riga {index}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_batch(current_batch_df, path_to_save_results):\n",
        "    batch_results_list = []\n",
        "    print(f\"    DEBUG process_batch: Inizio elaborazione di {len(current_batch_df)} righe...\")\n",
        "    for original_index, row_data in current_batch_df.iterrows():\n",
        "        single_row_result = process_row(original_index, row_data)\n",
        "        if single_row_result is not None:\n",
        "            batch_results_list.append(single_row_result)\n",
        "\n",
        "    print(f\"    DEBUG process_batch: Elaborazione batch completata. Ottenuti {len(batch_results_list)} risultati grezzi.\")\n",
        "    valid_results_for_saving = [r for r in batch_results_list if is_valid_final_output(r)]\n",
        "\n",
        "    if len(valid_results_for_saving) > 0:\n",
        "        if len(valid_results_for_saving) < len(batch_results_list):\n",
        "            print(f\"    DEBUG process_batch: ATTENZIONE - {len(batch_results_list) - len(valid_results_for_saving)} risultati grezzi non validi e scartati.\")\n",
        "        save_block_to_jsonl(valid_results_for_saving, path_to_save_results)\n",
        "        print(f\"    DEBUG process_batch: Batch salvato con {len(valid_results_for_saving)} risultati validi su {path_to_save_results}.\")\n",
        "    else:\n",
        "        print(f\"    DEBUG process_batch: Nessun risultato valido da salvare per questo batch.\")\n",
        "    return len(valid_results_for_saving)\n",
        "\n",
        "print(\"--- Funzioni di elaborazione definite. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8eb71bdf19fb41efa5087836cc3ac153",
            "ba0c3172c2dd4fea8df480f0c9dfd372",
            "8da878eaf70b422cbe5424af1c1fa506",
            "ac0e3a2d044b4f1cb5d08785267d1382",
            "8d462602ae364df2bd084e5ec4482bce",
            "03112045686f4a78997188cba1444053",
            "2e824cba482a4b8b9425c58a6c8000e5",
            "951c22e14bf74fb09860e087d24b523b",
            "78fc249087fb4fe49b8a45b5ed725f3b",
            "8c00b1eb75aa463ab11b53e080d14a4c",
            "4029b1cdcfa2454a994e1c1085c5dddf"
          ]
        },
        "id": "ho6L8rSGzZSH",
        "outputId": "7c0a1827-cc6c-461c-896d-6252f7c41876"
      },
      "outputs": [],
      "source": [
        "# prova con output che non stampa i debug ma la percentuale di completamento e che salva riga per riga senza aspettare la fine del batch\n",
        "# --- Blocco 3: Esecuzione Principale con Salvataggio Riga per Riga e Barra di Progresso ---\n",
        "from tqdm.auto import tqdm # Importa tqdm per la barra di progresso\n",
        "\n",
        "# Verifica che le variabili e funzioni cruciali siano definite\n",
        "if 'df' not in globals() and ('model' not in globals() or 'tokenizer' not in globals() or 'process_row' not in globals()):\n",
        "    print(\"ERRORE CRITICO: DataFrame 'df' O ('model', 'tokenizer', 'process_row') non definiti.\")\n",
        "    print(\"Esegui prima la CELLA 1 (Setup) e la CELLA 2 (Definizione Funzioni). Poi carica 'df'.\")\n",
        "    # Tenta di caricare df se non definito\n",
        "    try:\n",
        "        df_path = r\"/content/drive/MyDrive/data_giacomo.csv\"\n",
        "        print(f\"Tentativo di caricare il DataFrame da: {df_path}\")\n",
        "        df = pd.read_csv(df_path)\n",
        "        print(f\"DataFrame caricato con {len(df)} righe.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERRORE: File DataFrame '{df_path}' non trovato. Impossibile procedere.\")\n",
        "        raise SystemExit(\"DataFrame non trovato.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERRORE durante il caricamento del DataFrame: {e}\")\n",
        "        raise SystemExit(\"Errore caricamento DataFrame.\")\n",
        "elif 'df' not in globals(): # Se model, tokenizer, ecc. sono definiti ma df no\n",
        "    try:\n",
        "        df_path = r\"/content/drive/MyDrive/data_giacomo.csv\"\n",
        "        print(f\"Tentativo di caricare il DataFrame da: {df_path}\")\n",
        "        df = pd.read_csv(df_path)\n",
        "        print(f\"DataFrame caricato con {len(df)} righe.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERRORE: File DataFrame '{df_path}' non trovato. Impossibile procedere.\")\n",
        "        raise SystemExit(\"DataFrame non trovato.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERRORE durante il caricamento del DataFrame: {e}\")\n",
        "        raise SystemExit(\"Errore caricamento DataFrame.\")\n",
        "else:\n",
        "     print(f\"DataFrame 'df' ({len(df)} righe), Setup modello e funzioni OK. Procedo.\")\n",
        "\n",
        "\n",
        "results_path = \"/content/drive/MyDrive/results_patent_mistral7b_gpu.jsonl\"\n",
        "print(f\"\\nL'output verrà salvato riga per riga in: {results_path}\")\n",
        "print(\"Assicurati che le colonne 'patent', 'appln_title', 'appln_abstract' siano presenti nel DataFrame 'df'.\")\n",
        "\n",
        "NUMERO_RIGHE_DA_PROCESSARE_ORA = 400\n",
        "\n",
        "done_patent_ids = get_already_processed_patent_ids(results_path)\n",
        "print(f\"Trovati {len(done_patent_ids)} patent_id già processati nel file di output.\")\n",
        "\n",
        "if \"patent\" not in df.columns:\n",
        "    print(\"ERRORE: La colonna 'patent' non esiste nel DataFrame 'df'.\")\n",
        "else:\n",
        "    df_all_new_rows = df[~df[\"patent\"].astype(str).isin(done_patent_ids)]\n",
        "    print(f\"Numero di righe totali nel DataFrame 'df': {len(df)}\")\n",
        "    print(f\"Numero di righe nuove (non ancora processate): {len(df_all_new_rows)}\")\n",
        "\n",
        "    if len(df_all_new_rows) == 0:\n",
        "        print(\"Nessuna nuova riga da processare.\")\n",
        "    else:\n",
        "        df_this_run = df_all_new_rows.head(NUMERO_RIGHE_DA_PROCESSARE_ORA)\n",
        "\n",
        "        if len(df_this_run) == 0:\n",
        "            print(f\"Nessuna riga selezionata per questa esecuzione (limite: {NUMERO_RIGHE_DA_PROCESSARE_ORA} o nessuna riga nuova).\")\n",
        "        else:\n",
        "            print(f\"Verranno processate e salvate individualmente le prossime {len(df_this_run)} righe.\")\n",
        "\n",
        "            processed_count_in_this_run = 0\n",
        "            # Itera sulle righe selezionate per barra di progresso\n",
        "            for original_index, row_data in tqdm(df_this_run.iterrows(), total=len(df_this_run), desc=\"Processing rows\"):\n",
        "                try:\n",
        "                    # Processa la singola riga\n",
        "                    single_row_result = process_row(original_index, row_data)\n",
        "\n",
        "                    if single_row_result is not None:\n",
        "                        # Valida il risultato finale (include patent_id)\n",
        "                        if is_valid_final_output(single_row_result):\n",
        "                            # Salva il singolo risultato valido\n",
        "                            save_block_to_jsonl([single_row_result], results_path)\n",
        "                            processed_count_in_this_run += 1\n",
        "                except Exception as e_row_proc:\n",
        "                    print(f\"ERRORE grave durante l'elaborazione della riga con indice originale {original_index}: {e_row_proc}\")\n",
        "                    # import traceback; traceback.print_exc() # Per debug più approfondito\n",
        "\n",
        "            print(f\"\\n--- Elaborazione di {len(df_this_run)} righe richiesta completata. ---\")\n",
        "            print(f\"Salvati {processed_count_in_this_run} risultati validi in questo run.\")\n",
        "\n",
        "    print(f\"Controlla il file di output: {results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHhWyu59wARA",
        "outputId": "64fc2b48-3892-468e-9966-5fac07ba77f1"
      },
      "outputs": [],
      "source": [
        "# check per vedere se l output esiste:\n",
        "!ls -l /content/drive/MyDrive/results_patent_mistral7b_gpu.jsonl\n",
        "#!cat /content/drive/MyDrive/results_patent_mistral7b_gpu.jsonl # ATTENZIONE: stampa tutto il contenuto, usa con cautela se il file è grande"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1HxwSAR40Lw",
        "outputId": "db6c5d6e-4636-49ba-b049-d1799542ae39"
      },
      "outputs": [],
      "source": [
        "# pe controllare quanti risultati contiene il mio json di output\n",
        "import json\n",
        "import os # Aggiunto per controllare se il file esiste\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Monta Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount può essere utile\n",
        "    print(\"Google Drive montato con successo!\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il montaggio di Google Drive: {e}\")\n",
        "    raise SystemExit(\"Impossibile montare Google Drive. Verifica i permessi.\")\n",
        "\n",
        "def analizza_file_jsonl(percorso_file_jsonl):\n",
        "    \"\"\"\n",
        "    Analizza un file JSONL per contare le righe totali,\n",
        "    gli oggetti JSON validi, e gli ID brevetto unici.\n",
        "    \"\"\"\n",
        "    conteggio_righe_totali = 0\n",
        "    conteggio_oggetti_json_validi = 0\n",
        "    patent_ids_unici_nel_file = set()\n",
        "    errori_parsing_json = 0\n",
        "    righe_senza_patent_id = 0\n",
        "\n",
        "    print(f\"--- Analisi del file: {percorso_file_jsonl} ---\")\n",
        "\n",
        "    if not os.path.exists(percorso_file_jsonl):\n",
        "        print(f\"ERRORE: Il file specificato non è stato trovato: {percorso_file_jsonl}\")\n",
        "        return 0, 0, 0, 0, 0\n",
        "\n",
        "    try:\n",
        "        with open(percorso_file_jsonl, 'r', encoding='utf-8') as f:\n",
        "            for i, riga in enumerate(f):\n",
        "                conteggio_righe_totali += 1\n",
        "                try:\n",
        "                    obj = json.loads(riga)\n",
        "                    conteggio_oggetti_json_validi += 1\n",
        "                    if \"patent_id\" in obj and obj[\"patent_id\"] is not None:\n",
        "                        patent_ids_unici_nel_file.add(str(obj[\"patent_id\"]))\n",
        "                    else:\n",
        "                        righe_senza_patent_id += 1\n",
        "                        # print(f\"Riga {i+1} non contiene 'patent_id' o è None: {riga.strip()[:100]}...\") # Debug opzionale\n",
        "                except json.JSONDecodeError:\n",
        "                    errori_parsing_json += 1\n",
        "                    # print(f\"Riga {i+1} non è un JSON valido: {riga.strip()[:100]}...\") # Debug opzionale\n",
        "\n",
        "        print(f\"Numero totale di righe nel file: {conteggio_righe_totali}\")\n",
        "        print(f\"Numero di oggetti JSON validi parsati: {conteggio_oggetti_json_validi}\")\n",
        "        print(f\"Numero di ID brevetto ('patent_id') UNICI trovati: {len(patent_ids_unici_nel_file)}\")\n",
        "        if righe_senza_patent_id > 0:\n",
        "            print(f\"ATTENZIONE: {righe_senza_patent_id} oggetti JSON validi non avevano una chiave 'patent_id' o era None.\")\n",
        "        if errori_parsing_json > 0:\n",
        "            print(f\"ATTENZIONE: {errori_parsing_json} righe non sono state parsate correttamente come JSON.\")\n",
        "\n",
        "        return conteggio_righe_totali, conteggio_oggetti_json_validi, len(patent_ids_unici_nel_file)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Si è verificato un errore generale durante la lettura del file: {e}\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "# --- ESECUZIONE DELL'ANALISI ---\n",
        "percorso_del_tuo_output_jsonl = \"/content/drive/MyDrive/results_patent_mistral7b_gpu.jsonl\"\n",
        "#funzione di analisi\n",
        "tot_lines, valid_json_obj, unique_ids = analizza_file_jsonl(percorso_del_tuo_output_jsonl)\n",
        "\n",
        "print(\"\\n--- Riepilogo ---\")\n",
        "if tot_lines > 0:\n",
        "    print(f\"Il tuo file di output contiene {tot_lines} righe.\")\n",
        "    print(f\"Di queste, {valid_json_obj} sono state lette come oggetti JSON validi.\")\n",
        "    print(f\"All'interno di questi oggetti JSON validi, sono stati trovati {unique_ids} 'patent_id' unici.\")\n",
        "    print(\"\\nQuesto numero di 'patent_id' unici è quello che viene usato per calcolare le 'righe nuove da processare'.\")\n",
        "else:\n",
        "    if os.path.exists(percorso_del_tuo_output_jsonl):\n",
        "        print(\"Il file di output sembra essere vuoto o illeggibile.\")\n",
        "    else:\n",
        "        print(\"Il file di output non è stato trovato. Verifica il percorso.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03112045686f4a78997188cba1444053": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e824cba482a4b8b9425c58a6c8000e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4029b1cdcfa2454a994e1c1085c5dddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78fc249087fb4fe49b8a45b5ed725f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c00b1eb75aa463ab11b53e080d14a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d462602ae364df2bd084e5ec4482bce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da878eaf70b422cbe5424af1c1fa506": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951c22e14bf74fb09860e087d24b523b",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78fc249087fb4fe49b8a45b5ed725f3b",
            "value": 121
          }
        },
        "8eb71bdf19fb41efa5087836cc3ac153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba0c3172c2dd4fea8df480f0c9dfd372",
              "IPY_MODEL_8da878eaf70b422cbe5424af1c1fa506",
              "IPY_MODEL_ac0e3a2d044b4f1cb5d08785267d1382"
            ],
            "layout": "IPY_MODEL_8d462602ae364df2bd084e5ec4482bce"
          }
        },
        "951c22e14bf74fb09860e087d24b523b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0e3a2d044b4f1cb5d08785267d1382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c00b1eb75aa463ab11b53e080d14a4c",
            "placeholder": "​",
            "style": "IPY_MODEL_4029b1cdcfa2454a994e1c1085c5dddf",
            "value": " 121/400 [19:01&lt;38:03,  8.18s/it]"
          }
        },
        "ba0c3172c2dd4fea8df480f0c9dfd372": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03112045686f4a78997188cba1444053",
            "placeholder": "​",
            "style": "IPY_MODEL_2e824cba482a4b8b9425c58a6c8000e5",
            "value": "Processing rows:  30%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
